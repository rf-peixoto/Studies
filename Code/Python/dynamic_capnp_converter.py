#!/usr/bin/env python3

"""
Script: dynamic_capnp_converter.py

This script demonstrates:
1) Reading JSON or CSV files, discovering fields dynamically, and creating a
   temporary Cap'n Proto schema that matches those fields (treated as text).
2) Writing data into a Cap'n Proto file, embedding two parts:
     a) The generated schema text (for future reference).
     b) The actual data (based on that schema).
3) Reading Cap'n Proto files generated by this script, extracting the embedded
   schema, recompiling it dynamically, and then converting the data to JSON or CSV.

Usage Examples:
  1) JSON -> Cap'n Proto:
     python dynamic_capnp_converter.py --mode=json2capnp \
       --input=input.json --output=output.capnp

  2) CSV -> Cap'n Proto:
     python dynamic_capnp_converter.py --mode=csv2capnp \
       --input=input.csv --output=output.capnp

  3) Cap'n Proto -> JSON:
     python dynamic_capnp_converter.py --mode=capnp2json \
       --input=input.capnp --output=output.json

  4) Cap'n Proto -> CSV:
     python dynamic_capnp_converter.py --mode=capnp2csv \
       --input=input.capnp --output=output.csv

Dependencies:
    pip install pycapnp
"""

import argparse
import sys
import os
import json
import csv
import tempfile

import capnp  # The official import name in many distributions is "capnp" (pycapnp).

##############################################################################
#                            SCHEMA GENERATION
##############################################################################

def guess_fields_from_json(json_data):
    """
    Given a JSON array of objects or a single object, discover all field names.
    Returns a sorted list of unique fields.
    """
    if isinstance(json_data, dict):
        # Single object
        fields = set(json_data.keys())
    elif isinstance(json_data, list):
        # Array of objects
        fields = set()
        for row in json_data:
            if isinstance(row, dict):
                fields.update(row.keys())
    else:
        print("Error: JSON data must be a dict or list of dicts.")
        sys.exit(1)

    return sorted(fields)

def guess_fields_from_csv(csv_path):
    """
    Given a CSV file, read headers, return a sorted list of field names.
    """
    with open(csv_path, 'r', encoding='utf-8', newline='') as fp:
        reader = csv.reader(fp)
        headers = next(reader, None)
    if headers is None:
        print("Error: CSV file is empty or invalid.")
        sys.exit(1)
    return sorted(headers)

def build_dynamic_schema_str(struct_name, fields):
    """
    Build a Cap'n Proto schema text that has:
      struct_name: the name of the struct to store a single row/record
      fields: a list of strings for field names
    We treat each field as Text (string) for simplicity.
    Also, we define a top-level struct DataSet containing:
      1) the embedded schema text (schemaText) for future reference
      2) a list of struct_name records (records)
    """
    schema_lines = []
    schema_lines.append("@0xabcdefabcdefabcdef;")  # A dummy unique ID for the file
    schema_lines.append(f"struct {struct_name} {{")
    # Assign each field an index. All are text for simplicity.
    for index, field in enumerate(fields):
        # In Cap'n Proto, field names must be valid identifiers (no spaces, special chars).
        # We do a naive approach here. Real usage should sanitize or transform them.
        sanitized = sanitize_field_name(field)
        schema_lines.append(f"  {sanitized} @{index} :Text;")
    schema_lines.append("}")

    # A top-level container that holds the dynamic schema text and the actual data
    schema_lines.append("struct DataSet {")
    schema_lines.append("  schemaText @0 :Text;")
    schema_lines.append(f"  records @1 :List({struct_name});")
    schema_lines.append("}")

    return "\n".join(schema_lines)

def sanitize_field_name(field_name):
    """
    Convert field_name into a valid Cap'n Proto identifier.
    This is a minimal version: only keep [a-zA-Z0-9_], replace others with '_'.
    """
    valid_chars = []
    for c in field_name:
        if c.isalnum() or c == '_':
            valid_chars.append(c)
        else:
            valid_chars.append('_')
    # Cap'n Proto fields must not start with a digit
    if valid_chars and valid_chars[0].isdigit():
        valid_chars.insert(0, '_')
    return "".join(valid_chars)

##############################################################################
#                             WRITE  (JSON/CSV -> Cap'n Proto)
##############################################################################

def json_to_capnp(input_file, output_file):
    """
    1. Read JSON from input_file
    2. Guess fields
    3. Build dynamic schema, compile it, create a message
    4. Write the schema text + data into the output .capnp file
    """
    with open(input_file, 'r', encoding='utf-8') as fp:
        data = json.load(fp)
    fields = guess_fields_from_json(data)

    schema_str = build_dynamic_schema_str("Row", fields)

    # Compile from string
    schema = capnp.SchemaParser().parse(schema_str)

    DataSet = schema.DataSet
    Row = schema.Row

    # Build the message
    message = DataSet.new_message()
    message.schemaText = schema_str  # embed the dynamic schema in the message

    # If it's a single dict, wrap it in a list
    if isinstance(data, dict):
        data = [data]

    records = message.init("records", len(data))
    for i, row in enumerate(data):
        for field in fields:
            sanitized = sanitize_field_name(field)
            value = row.get(field, "")
            records[i]._set_str(sanitized, str(value))  # store as string

    with open(output_file, 'wb') as fp:
        message.write(fp)

def csv_to_capnp(input_file, output_file):
    """
    1. Read CSV from input_file
    2. Guess fields from header
    3. Build dynamic schema, compile it, create a message
    4. Write the schema text + data into the output .capnp file
    """
    fields = guess_fields_from_csv(input_file)
    schema_str = build_dynamic_schema_str("Row", fields)

    schema = capnp.SchemaParser().parse(schema_str)
    DataSet = schema.DataSet
    Row = schema.Row

    with open(input_file, 'r', encoding='utf-8', newline='') as fp:
        reader = csv.DictReader(fp)
        rows = list(reader)

    message = DataSet.new_message()
    message.schemaText = schema_str
    records = message.init("records", len(rows))

    for i, row in enumerate(rows):
        for field in fields:
            sanitized = sanitize_field_name(field)
            value = row.get(field, "")
            records[i]._set_str(sanitized, str(value))

    with open(output_file, 'wb') as fp:
        message.write(fp)

##############################################################################
#                             READ  (Cap'n Proto -> JSON/CSV)
##############################################################################

def capnp_to_json(input_file, output_file):
    """
    1. Read the Cap'n Proto file
    2. Extract the embedded schema text
    3. Dynamically parse it, read the data
    4. Write JSON
    """
    with open(input_file, 'rb') as fp:
        # We do not know the schema yet, but we can parse as "AnyPointer" first:
        data = fp.read()

    # Step A: read the top-level container "DataSet" using a "blind" approach
    # with a minimal known schema
    minimal_schema_str = """
        @0xabcdefabcdefabcdef;
        struct DataSet {
          schemaText @0 :Text;
          records @1 :List(AnyPointer);
        }
    """
    minimal_schema = capnp.SchemaParser().parse(minimal_schema_str)
    minimal_DataSet = minimal_schema.DataSet

    # Create a message from data in a "blind" manner
    msg = minimal_DataSet.from_bytes(data)

    # Extract the actual schema text
    embedded_schema_text = msg.schemaText

    # Now parse the real, embedded schema:
    real_schema = capnp.SchemaParser().parse(embedded_schema_text)
    real_DataSet = real_schema.DataSet
    real_Row = real_schema.Row

    # Re-read the entire message with the real schema
    # Because we have to interpret the bytes again, properly:
    parsed_msg = real_DataSet.from_bytes(data)

    # Extract fields from the embedded Row struct
    # This time, we can look at the schema's fields
    row_schema = real_Row.schema()
    row_fields = [f.name for f in row_schema.fields]
    records = parsed_msg.records

    # Convert to Python data
    result_data = []
    for record in records:
        record_dict = {}
        for field_name in row_fields:
            # Each field is text (string)
            record_dict[field_name] = getattr(record, field_name)
        result_data.append(record_dict)

    # Write to JSON
    with open(output_file, 'w', encoding='utf-8') as fp:
        json.dump(result_data, fp, indent=2)

def capnp_to_csv(input_file, output_file):
    """
    1. Read the Cap'n Proto file
    2. Extract the embedded schema text
    3. Dynamically parse it, read the data
    4. Write CSV
    """
    with open(input_file, 'rb') as fp:
        data = fp.read()

    # Same minimal approach to read the top-level container
    minimal_schema_str = """
        @0xabcdefabcdefabcdef;
        struct DataSet {
          schemaText @0 :Text;
          records @1 :List(AnyPointer);
        }
    """
    minimal_schema = capnp.SchemaParser().parse(minimal_schema_str)
    minimal_DataSet = minimal_schema.DataSet
    msg = minimal_DataSet.from_bytes(data)

    embedded_schema_text = msg.schemaText

    # Parse the real schema
    real_schema = capnp.SchemaParser().parse(embedded_schema_text)
    real_DataSet = real_schema.DataSet
    real_Row = real_schema.Row

    parsed_msg = real_DataSet.from_bytes(data)
    row_schema = real_Row.schema()
    row_fields = [f.name for f in row_schema.fields]

    records = parsed_msg.records

    # Write CSV
    with open(output_file, 'w', encoding='utf-8', newline='') as fp:
        writer = csv.writer(fp)
        # Write header
        writer.writerow(row_fields)
        for record in records:
            row_data = []
            for field_name in row_fields:
                row_data.append(getattr(record, field_name))
            writer.writerow(row_data)

##############################################################################
#                                    MAIN
##############################################################################

def main():
    parser = argparse.ArgumentParser(description="Dynamic Cap'n Proto Converter")
    parser.add_argument("--mode", required=True, type=str,
                        help="One of: json2capnp, csv2capnp, capnp2json, capnp2csv")
    parser.add_argument("--input", required=True, type=str,
                        help="Path to input JSON/CSV/Cap'n Proto file")
    parser.add_argument("--output", required=True, type=str,
                        help="Path to output file")

    args = parser.parse_args()
    mode = args.mode.lower()

    if mode == "json2capnp":
        json_to_capnp(args.input, args.output)
    elif mode == "csv2capnp":
        csv_to_capnp(args.input, args.output)
    elif mode == "capnp2json":
        capnp_to_json(args.input, args.output)
    elif mode == "capnp2csv":
        capnp_to_csv(args.input, args.output)
    else:
        print("Error: Invalid mode.")
        sys.exit(1)

    print("Operation complete.")

if __name__ == "__main__":
    main()
